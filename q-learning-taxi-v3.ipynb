{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-18T16:52:19.385744Z",
     "iopub.status.busy": "2021-03-18T16:52:19.384733Z",
     "iopub.status.idle": "2021-03-18T16:52:19.388044Z",
     "shell.execute_reply": "2021-03-18T16:52:19.387201Z"
    },
    "papermill": {
     "duration": 0.024888,
     "end_time": "2021-03-18T16:52:19.388285",
     "exception": false,
     "start_time": "2021-03-18T16:52:19.363397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T16:52:19.416190Z",
     "iopub.status.busy": "2021-03-18T16:52:19.415347Z",
     "iopub.status.idle": "2021-03-18T16:52:19.418353Z",
     "shell.execute_reply": "2021-03-18T16:52:19.417745Z"
    },
    "papermill": {
     "duration": 0.018933,
     "end_time": "2021-03-18T16:52:19.418519",
     "exception": false,
     "start_time": "2021-03-18T16:52:19.399586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install cmake 'gym[atari]' scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T16:52:19.446473Z",
     "iopub.status.busy": "2021-03-18T16:52:19.445563Z",
     "iopub.status.idle": "2021-03-18T16:52:20.774129Z",
     "shell.execute_reply": "2021-03-18T16:52:20.773224Z"
    },
    "papermill": {
     "duration": 1.345066,
     "end_time": "2021-03-18T16:52:20.774387",
     "exception": false,
     "start_time": "2021-03-18T16:52:19.429321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T16:52:20.809211Z",
     "iopub.status.busy": "2021-03-18T16:52:20.807528Z",
     "iopub.status.idle": "2021-03-18T16:52:20.812593Z",
     "shell.execute_reply": "2021-03-18T16:52:20.813412Z"
    },
    "papermill": {
     "duration": 0.026358,
     "end_time": "2021-03-18T16:52:20.813709",
     "exception": false,
     "start_time": "2021-03-18T16:52:20.787351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: |\u001b[43m \u001b[0m: :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "# env.reset: Resets the environment and returns a random initial state.\n",
    "# env.step(action): Step the environment by one timestep. Returns\n",
    "\n",
    "env.reset() # reset environment to a new, random state\n",
    "env.render() # renders one frame of the environment (helpful in visualizing the environment)\n",
    "\n",
    "#passenger at blue marker, drop at purple marker\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T16:52:20.847857Z",
     "iopub.status.busy": "2021-03-18T16:52:20.846797Z",
     "iopub.status.idle": "2021-03-18T16:52:20.851378Z",
     "shell.execute_reply": "2021-03-18T16:52:20.850695Z"
    },
    "papermill": {
     "duration": 0.023731,
     "end_time": "2021-03-18T16:52:20.851558",
     "exception": false,
     "start_time": "2021-03-18T16:52:20.827827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  (5 rows, 5 columns, 5 passenger positions (any one of the 4 markers or in the car), 4 drop positions) = 500 states\n",
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state # manually setting the state of the environment\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01136,
     "end_time": "2021-03-18T16:52:20.875429",
     "exception": false,
     "start_time": "2021-03-18T16:52:20.864069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Actions :\n",
    "* 0 = south\n",
    "* 1 = north\n",
    "* 2 = east\n",
    "* 3 = west\n",
    "* 4 = pickup\n",
    "* 5 = dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T16:52:20.911205Z",
     "iopub.status.busy": "2021-03-18T16:52:20.910159Z",
     "iopub.status.idle": "2021-03-18T16:52:20.914830Z",
     "shell.execute_reply": "2021-03-18T16:52:20.914147Z"
    },
    "papermill": {
     "duration": 0.026679,
     "end_time": "2021-03-18T16:52:20.914994",
     "exception": false,
     "start_time": "2021-03-18T16:52:20.888315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial reward table - a states × actions matrix.\n",
    "env.P[328]\n",
    "# this dictionary has the structure {action: [(probability, nextstate, reward, done)]}\n",
    "# done is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012299,
     "end_time": "2021-03-18T16:52:20.939690",
     "exception": false,
     "start_time": "2021-03-18T16:52:20.927391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Completing the task without Reinforcement Learning\n",
    "\n",
    "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The env.action_space.sample() method automatically selects one random action from set of all possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T16:52:20.981414Z",
     "iopub.status.busy": "2021-03-18T16:52:20.976457Z",
     "iopub.status.idle": "2021-03-18T16:52:21.875299Z",
     "shell.execute_reply": "2021-03-18T16:52:21.874502Z"
    },
    "papermill": {
     "duration": 0.922543,
     "end_time": "2021-03-18T16:52:21.875475",
     "exception": false,
     "start_time": "2021-03-18T16:52:20.952932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps taken: 13522\n",
      "Penalties incurred: 4378\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T16:52:21.915212Z",
     "iopub.status.busy": "2021-03-18T16:52:21.913991Z",
     "iopub.status.idle": "2021-03-18T17:15:19.726833Z",
     "shell.execute_reply": "2021-03-18T17:15:19.727642Z"
    },
    "papermill": {
     "duration": 1377.837851,
     "end_time": "2021-03-18T17:15:19.727971",
     "exception": false,
     "start_time": "2021-03-18T16:52:21.890120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 13522\n",
      "State: 0\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "        \n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012695,
     "end_time": "2021-03-18T17:15:19.758079",
     "exception": false,
     "start_time": "2021-03-18T17:15:19.745384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implementing Q Learning in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T17:15:19.790256Z",
     "iopub.status.busy": "2021-03-18T17:15:19.789609Z",
     "iopub.status.idle": "2021-03-18T17:15:19.792702Z",
     "shell.execute_reply": "2021-03-18T17:15:19.792173Z"
    },
    "papermill": {
     "duration": 0.021901,
     "end_time": "2021-03-18T17:15:19.792867",
     "exception": false,
     "start_time": "2021-03-18T17:15:19.770966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012764,
     "end_time": "2021-03-18T17:15:19.819103",
     "exception": false,
     "start_time": "2021-03-18T17:15:19.806339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxQ(next state,all actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T17:15:19.899571Z",
     "iopub.status.busy": "2021-03-18T17:15:19.887270Z",
     "iopub.status.idle": "2021-03-18T17:16:29.826628Z",
     "shell.execute_reply": "2021-03-18T17:16:29.826025Z"
    },
    "papermill": {
     "duration": 69.994393,
     "end_time": "2021-03-18T17:16:29.826792",
     "exception": false,
     "start_time": "2021-03-18T17:15:19.832399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 1min 11s, sys: 17.7 s, total: 1min 29s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013755,
     "end_time": "2021-03-18T17:16:29.855017",
     "exception": false,
     "start_time": "2021-03-18T17:16:29.841262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-18T17:16:29.896715Z",
     "iopub.status.busy": "2021-03-18T17:16:29.894593Z",
     "iopub.status.idle": "2021-03-18T17:16:29.968194Z",
     "shell.execute_reply": "2021-03-18T17:16:29.967538Z"
    },
    "papermill": {
     "duration": 0.099539,
     "end_time": "2021-03-18T17:16:29.968356",
     "exception": false,
     "start_time": "2021-03-18T17:16:29.868817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.82\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    frames = [] #for animation\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "        \n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "        \n",
    "#     print_frames(frames) #for animation\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1458.134768,
   "end_time": "2021-03-18T17:16:30.898666",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-18T16:52:12.763898",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
